# Pachyderm v2

## Notable new features

### A diff-oriented processing model

In v1, PPS only processes additions.  That is, if an input commit modifies/deletes data, PPS only processes the positive diffs, but ignores the negative diffs.

In v2, PPS processes diffs the way you'd expect.  For instance, if you remove some input data, downstream pipelines will know to remove the output that were produced as a result of processing the said input data.

### A vastly simplified and more powerful partition scheme

In v1, the way input data is partitioned among parallel pods is determined by two flags: "partition" and "incremental".  These flags have been a pain point because they are not intuitive (quick test, what does `partition=DIR` and `incremental=FULL` mean exactly?) and are not powerful enough to allow for some fairly common usage patterns (e.g. when you update a file `/dir/foo`, you want to reprocess the entire directory `/dir`).

In v2, you control how the input data is partitioned by specifying a single [glob](http://man7.org/linux/man-pages/man7/glob.7.html) pattern.  A couple examples:

* Partition objects under the root directory:`*` or `/*`
* Partition objects under directory `/dir`: `/dir/*`
* Process the entire repo for each commit: `` or `/`
* Process the entire directory `/dir` for each commit: `/dir`

Essentially, you can imagine that you are using an infinitely file system, and the input data that you want to process is selected via a single glob pattern.

This also implies that a pipeline can process only a subset of an input repo.  For instance, You might have two pipelines that take the same repo as input, but have different glob patterns:

* Pipeline 1: `/foo/*`
* Pipeline 2: `/bar/*`

In this case, if something under `/foo` changed, pipeline 1 gets triggered.  If something under `/bar` changed, pipeline 2 gets triggered.

## Persistent Store

Under the new architecture, all persistent state is stored under etcd (v3).  We pick etcd for the following reasons:

* Support for multi-object transactions.  This will get rid of a host of inconsistency issues that we've occasionally run into (e.g. commit size doesn't match all file sizes adding up).
* Support for a "[watch](https://coreos.com/etcd/docs/latest/api.html#waiting-for-a-change)" mechanism.  Watch is similar to RethinkDB's changefeed in that it's a way for clients to be notified of server-side events (e.g. a commit is added).  However, it's much more robust, primarily because it's implemented with long-polling, as opposed to changefeeds which basically "fire and forget", meaning that messages are sent at most once and can be lost due to network issues. 
* Known to work on Kubernetes.  In fact, Kubernetes itself stores all persistent state in etcd.
* In-house expertise.  We are already using etcd for PPS's sharding mechanism, so we are fairly familiar with the software.
* Maturity and active development.  etcd is being used by some of the largest cloud software such as Kubernetes itself.  It's also being actively developed in the open (in constrast with Rethink, which hasn't seen a commit since the company shuts down).

## PFS

### Repos

Each repo's metadata is stored as an object with the prefix `/repos/`.

A repo's size is defined as the size of all trees referenced by this repo.

#### CreateRepo(name)

```
put /repos/name <metadata>
```

#### InspectRepo(name)

`get /repos/name`

#### ListRepo

`get -prefix /repos/`

#### DeleteRepo(name)

```
txnBegin
  delete /repos/name
  delete -prefix /commits/name/
txnEnd
```

### Commits

A commit's size is defined as the total size of all the trees it references.  Logically, it's the size of the snapshot of the repo that the commit represents.

#### StartCommit(repo, ref, parent, provenance)

start-commit syntax:

start-commit repo master
conceptually equivlanet to: start-commit repo master -p master

start-commit repo experimental -p master

```
txnBegin
  t := <new-uuid>
  p = nil
  if parent == nil {
    r := get /refs/repo/ref
    if r != nil {
      p = get /commits/repo/r
    }
  } else {
    p = get /commits/repo/parent
  }
  c := <new-commit>
  if p != nil {
    c.trees = p.trees + t
    <optionally compact c.trees>
  } else {
    c.trees = t
  }
  commitID := <new-uuid>
  put /commits/repo/commitID c 
  put /refs/repo/ref commitID
  fullProvenance = provenance
  for p in provenance:
    fullProvenance += list /ecnanevorp/p
  for p in fullProvenance:
    put /provenance/p commitID
txnEnd
```

#### FinishCommit(repo, commit)

FinishCommit locks the active tree and computes its size.

```
txnBegin
  c := get /commits/repo/commit
txnEnd
```

#### SubscribeCommit(repo, sinceCommit)

```
c := get /commits/repo/sinceCommit
for c in <watch -prefix /commits/repo/ -sort=createRevision -rev=c.CreateRev>:
  send(c)
}
```

#### ListCommit(repo, ref, n)

// list commits in foo
list-commit foo
// list commits in foo that are ancestors of master
// i.e. list commits on branch master
list-commit foo master
// list commits in foo that are ancestors of master~10
list-commit foo master~10
// list commits in foo that are between master~10 and master
list-commit foo master~10..master

```
r := get /refs/repo/ref
c := get /commits/repo/r
while n-- >= 0 {
  send(c)
  c = get /commits/repo/c.parent
}
```

### Files

From a high level, all read-only file operations (i.e. all except for PutFile) operate on an in-memory, content-addressed merkle directory tree.

These merkle trees, once constructed, are put into the block store.  There is a distributed cache (groupcache) in front of the block store, so naturally the trees will be cached in the distributed cache.

#### PutFile(repo, commit, path, data)

This would split a big file into multiple small files, each of which contains 1000 lines:

`put-file big_log --split=line -n 1000`

The small files would look like: `big_file/0`, `big_file/1`, `big_file/2`, etc.

```
txnBegin
  bloblist := putBlob(data)
  c := get /commits/repo/commit
  activeTree := c.trees.last
  either:
    p := get /trees/activeTree/p
    p += data
    put /trees/activeTree/p
  or:
    put /trees/activeTree/p/<nextSequentialKey> bloblist
txnEnd
```

nextSequentialKey can be implemented using transactions.  See: https://github.com/coreos/etcd/blob/master/contrib/recipes/key.go#L77

look into WithFirstKey, WithLastKey, WithSerializable, and WithSort

#### GetFile(repo, commit, path)

txnBegin
  if commit in cache:
    return cache.GetFile(commit, path)
  else:
    go build_up_cache()
    c := /commits/repo/commit
    blockrefs = []
    for t in c.trees:
      for bloblist in <get -prefix /trees/t/path -sort=key>:
        blockrefs += bloblist
    return blockrefs
txnEnd

#### GlobFile(repo, commit, pattern)

if commit not in cache:
  build_up_cache(commit)
cache.get(commit).glob(pattern)

#### ListFile(repo, commit, path)

txnBegin
  if commit in cache:
    return cache.ListFile(commit, path)
  else:
    build_up_cache()
    recurse
txnEnd

#### ListFileRecursive(repo, commit, path)

## PPS

PPS uses a general architecture where the existence of an entity is established through inserting an entry into etcd, then the entity is launched by one of the servers determined through sharding.

For instance, to create a pipeline, a key is inserted under `/pipelines`.  All PPS nodes watch the prefix `/pipelines`, and when they receive a new entry, they hash the entry to obtain a shard number.  If a node's assigned the corresponding shard, the node launches a process that runs the pipeline.  Jobs are created and managed in the same fashion. 

The pod commit that processed a datum should have the datum's content hash as its ID.  That way we don't even need a `datum hash -> pod commit` mapping.  A pod would just try to start the commmit, and if it fails, that means the datum has already been processed.

### pipelineManager(pipeline)

Datum filters are stored under `/jobs`

```
/jobs/<job id>/chunks/<uuid>
```

Each chunk contains the following information:

* Input commits
* Filter for each commit

Chunks are claimed using etcd leases:

```
/jobs/<job id>/leases/<uuid>
```

When a lease exists, that means the corresponding chunk is being worked on.  Leases are revoked automatically if their owners stop sending KeepAlive messages.

```
streams = []
for i in pipeline.InputRepos:
  streams += pfs.SubscribeCommit(i)
commitSetStream = getCommitSetStream(streams)
// note that the commit sets may be out of order; but thatâ€™s fine since
// we can run jobs out of order.
filters = gen_filters(parallelism)
txnBegin
  for filter in filters:
    enqueue /jobs/<job id>/chunks filter
txnEnd
```

### jobManager



## job-shim

nextSequentialChunk is a function that generates the smallest number between [0, n) where n is the total number of chunks, provided that the number does not already exists.  The chunk is guarded by a lease.

For instance, if chunk 0, 1, 2 exist, and n is 10, then nextSequentialChunk generates chunk 3.

If chunk 0, 2, 3 exist (because chunk 1 was lost), then nextSequentialChunk generates chunk 1.

Question: how do you bound the maximum number of retries for a chunk?

```
for True:
  chunk = nextSequentialChunk(/jobs/<job id>/chunks)
  go keepalive(/jobs/<job id>/chunks/chunk.ID)
  for datum in pfs.GlobFile(chunk.filter):
    pfs.StartCommit(hash(datum))
    download(datum)
    run()
    upload()
    pfs.FinishCommit(hash(datum))
    clear_data()
  // putting the chunk again without a lease will override the lease, thus
  // effectively persisting the key
  put /jobs/<job id>/chunks/chunk.ID
```

---------------------------------------------------

Use `get /chunks --with-first-rev` to get a chunk.  Chunks are periodically refreshed by their owners, so the chunks that haven't been refreshed in a while tend to have low revs.

---------------------------------------------------

The problem: ideally we want to be able to take a key out of etcd, and when we die the key is automatically put back into etcd.  But that's not doable.  That's like an anti-lease.  Thus the following workaround.

(Note: a simpler solution where you only have one queue and you take an item by creating a lock is possible; however this solution gets unscalable when you have a large number of items in the queue, since there's no easy way to tell etcd to give you an item that does not have a corresponding lock.)

Logically we have a "work" queue and a "running" queue.  The work queue can be extremely large, whereas the running queue scales with parallelism and therefore is much smaller.

Workers start by transfering items from the work queue to the running queue in a transaction.  When a worker transfers an item to the running queue, it also creats a "lock" item.  Thus a worker creates the following two items in the running queue:

1. A "work" item, which is the same item as the one it grabbed from the work queue.
2. A "work.lock" item, which is guarded by a lease.

If a worker dies, it will then leave a "work" item in the running queue.  Thus, a worker is expected to look into the running queue and grab a work item once the original "work" queue has been exhausted.  To grab an item from the running queue, you attempt to lock it by creating a lease-guarded "work.lock" item.

The "work" item in the running queue can also keep information such as the number of retries that have been done. 

Furthermore, the "work" item in the running queue can be deleted once the worker has successfully processed the item, so as to keep the running queue small.

Question: how do you tell when a job has completed?

Answer: when there's no more items in the original work queue!

In our case, we can optimize the work queue by making it a counter instead.  So the counter starts at nil.  Then, the first worker would increment it to 0, and adds "0" and "0.lock" to the running queue.  The second worker would do that for "1", etc.

The work queue is considered exhausted when the counter hits the maximum number of datums.

In actuality, each processing unit is a filter as opposed to a datum.  So the counter really means: the m th 1/n input, where n is the filter modulus and m is the value of the counter.   In case of multiple inputs, use the base converstion algorithm. 

Question: you don't want to have O(n) commits where n is the number of datums, do you?  Ideally, a commit would map to multiple datums, and when a datum changed, you would be able to identify the commit that needs to be "recomputed".

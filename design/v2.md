# Pachyderm v2

## Notable new features

### A diff-oriented processing model

In v1, PPS only processes additions.  That is, if an input commit modifies/deletes data, PPS only processes the positive diffs, but ignores the negative diffs.

In v2, PPS processes diffs the way you'd expect.  For instance, if you remove some input data, downstream pipelines will know to remove the output that were produced as a result of processing the said input data.

### A vastly simplified and more powerful partition scheme

In v1, the way input data is partitioned among parallel pods is determined by two flags: "partition" and "incremental".  These flags have been a pain point because they are not intuitive (quick test, what does `partition=DIR` and `incremental=FULL` mean exactly?) and are not powerful enough to allow for some fairly common usage patterns (e.g. when you update a file `/dir/foo`, you want to reprocess the entire directory `/dir`).

In v2, you control how the input data is partitioned by specifying a single [glob](http://man7.org/linux/man-pages/man7/glob.7.html) pattern.  A couple examples:

* Partition objects under the root directory:`*` or `/*`
* Partition objects under directory `/dir`: `/dir/*`
* Process the entire repo for each commit: `` or `/`
* Process the entire directory `/dir` for each commit: `/dir`

Essentially, you can imagine that you are using an infinitely file system, and the input data that you want to process is selected via a single glob pattern.

This also implies that a pipeline can process only a subset of an input repo.  For instance, You might have two pipelines that take the same repo as input, but have different glob patterns:

* Pipeline 1: `/foo/*`
* Pipeline 2: `/bar/*`

In this case, if something under `/foo` changed, pipeline 1 gets triggered.  If something under `/bar` changed, pipeline 2 gets triggered.

## Persistent Store

Under the new architecture, all persistent state is stored under etcd (v3).  We pick etcd for the following reasons:

* Support for multi-object transactions.  This will get rid of a host of inconsistency issues that we've occasionally run into (e.g. commit size doesn't match all file sizes adding up).
* Support for a "[watch](https://coreos.com/etcd/docs/latest/api.html#waiting-for-a-change)" mechanism.  Watch is similar to RethinkDB's changefeed in that it's a way for clients to be notified of server-side events (e.g. a commit is added).  However, it's much more robust, primarily because it's implemented with long-polling, as opposed to changefeeds which basically "fire and forget", meaning that messages are sent at most once and can be lost due to network issues. 
* Known to work on Kubernetes.  In fact, Kubernetes itself stores all persistent state in etcd.
* In-house expertise.  We are already using etcd for PPS's sharding mechanism, so we are fairly familiar with the software.
* Maturity and active development.  etcd is being used by some of the largest cloud software such as Kubernetes itself.  It's also being actively developed in the open (in constrast with Rethink, which hasn't seen a commit since the company shuts down).

## PFS

### Repos

Each repo's metadata is stored as an object with the prefix `/repos/`.

A repo's size is defined as the size of all trees referenced by this repo.

#### CreateRepo(name)

```
put /repos/name <metadata>
```

#### InspectRepo(name)

`get /repos/name`

#### ListRepo

`get -prefix /repos/`

#### DeleteRepo(name)

```
txnBegin
  delete /repos/name
  delete -prefix /commits/name/
txnEnd
```

### Commits

A commit's size is defined as the total size of all the trees it references.  Logically, it's the size of the snapshot of the repo that the commit represents.

#### StartCommit(repo, ref, parent, provenance)

start-commit syntax:

start-commit repo master
conceptually equivlanet to: start-commit repo master -p master

start-commit repo experimental -p master

```
txnBegin
  t := <new-uuid>
  p = nil
  if parent == nil {
    r := get /refs/repo/ref
    if r != nil {
      p = get /commits/repo/r
    }
  } else {
    p = get /commits/repo/parent
  }
  c := <new-commit>
  if p != nil {
    c.trees = p.trees + t
    <optionally compact c.trees>
  } else {
    c.trees = t
  }
  commitID := <new-uuid>
  put /commits/repo/commitID c 
  put /refs/repo/ref commitID
  fullProvenance = provenance
  for p in provenance:
    fullProvenance += list /ecnanevorp/p
  for p in fullProvenance:
    put /provenance/p commitID
txnEnd
```

#### FinishCommit(repo, commit)

FinishCommit locks the active tree and computes its size.

```
txnBegin
  c := get /commits/repo/commit
txnEnd
```

#### SubscribeCommit(repo, sinceCommit)

```
c := get /commits/repo/sinceCommit
for c in <watch -prefix /commits/repo/ -sort=createRevision -rev=c.CreateRev>:
  send(c)
}
```

#### ListCommit(repo, ref, n)

// list commits in foo
list-commit foo
// list commits in foo that are ancestors of master
// i.e. list commits on branch master
list-commit foo master
// list commits in foo that are ancestors of master~10
list-commit foo master~10
// list commits in foo that are between master~10 and master
list-commit foo master~10..master

```
r := get /refs/repo/ref
c := get /commits/repo/r
while n-- >= 0 {
  send(c)
  c = get /commits/repo/c.parent
}
```

### Files

From a high level, all read-only file operations (i.e. all except for PutFile) operate on an in-memory, content-addressed merkle directory tree.

These merkle trees, once constructed, are put into the block store.  There is a distributed cache (groupcache) in front of the block store, so naturally the trees will be cached in the distributed cache.

#### PutFile(repo, commit, path, data)

This would split a big file into multiple small files, each of which contains 1000 lines:

`put-file big_log --split=line -n 1000`

The small files would look like: `big_file/0`, `big_file/1`, `big_file/2`, etc.

```
txnBegin
  bloblist := putBlob(data)
  c := get /commits/repo/commit
  activeTree := c.trees.last
  either:
    p := get /trees/activeTree/p
    p += data
    put /trees/activeTree/p
  or:
    put /trees/activeTree/p/<nextSequentialKey> bloblist
txnEnd
```

nextSequentialKey can be implemented using transactions.  See: https://github.com/coreos/etcd/blob/master/contrib/recipes/key.go#L77

look into WithFirstKey, WithLastKey, WithSerializable, and WithSort

#### GetFile(repo, commit, path)

txnBegin
  if commit in cache:
    return cache.GetFile(commit, path)
  else:
    go build_up_cache()
    c := /commits/repo/commit
    blockrefs = []
    for t in c.trees:
      for bloblist in <get -prefix /trees/t/path -sort=key>:
        blockrefs += bloblist
    return blockrefs
txnEnd

#### GlobFile(repo, commit, pattern)

if commit not in cache:
  build_up_cache(commit)
cache.get(commit).glob(pattern)

#### ListFile(repo, commit, path)

txnBegin
  if commit in cache:
    return cache.ListFile(commit, path)
  else:
    build_up_cache()
    recurse
txnEnd

#### ListFileRecursive(repo, commit, path)

## PPS

The pod commit that processed a datum should have the datum's content hash as its ID.  That way we don't even need a `datum hash -> pod commit` mapping.  A pod would just try to start the commmit, and if it fails, that means the datum has already been processed.

### pipelineManager(pipeline)

Datum filters are stored under `/jobs`

```
/jobs/<job id>/chunks/<uuid>
```

Each chunk contains the following information:

* Input commits
* Filter for each commit

Chunks are claimed using etcd leases:

```
/jobs/<job id>/leases/<uuid>
```

When a lease exists, that means the corresponding chunk is being worked on.  Leases are revoked automatically if their owners stop sending KeepAlive messages.

```
streams = []
for i in pipeline.InputRepos:
  streams += pfs.SubscribeCommit(i)
commitSetStream = getCommitSetStream(streams)
// note that the commit sets may be out of order; but thatâ€™s fine since
// we can run jobs out of order.
filters = gen_filters(parallelism)
txnBegin
  for filter in filters:
    enqueue /jobs/<job id>/chunks filter
txnEnd
```

### jobManager



## job-shim

nextSequentialChunk is a function that generates the smallest number between [0, n) where n is the total number of chunks, provided that the number does not already exists.  The chunk is guarded by a lease.

For instance, if chunk 0, 1, 2 exist, and n is 10, then nextSequentialChunk generates chunk 3.

If chunk 0, 2, 3 exist (because chunk 1 was lost), then nextSequentialChunk generates chunk 1.

Question: how do you bound the maximum number of retries for a chunk?

```
for True:
  chunk = nextSequentialChunk(/jobs/<job id>/chunks)
  go keepalive(/jobs/<job id>/chunks/chunk.ID)
  for datum in pfs.GlobFile(chunk.filter):
    pfs.StartCommit(hash(datum))
    download(datum)
    run()
    upload()
    pfs.FinishCommit(hash(datum))
    clear_data()
  // putting the chunk again without a lease will override the lease, thus
  // effectively persisting the key
  put /jobs/<job id>/chunks/chunk.ID
```
